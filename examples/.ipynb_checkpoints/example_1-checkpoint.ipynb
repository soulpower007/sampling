{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson Disk Sampling\n",
    "\n",
    "\n",
    "This module is an implementation of the active mini-batch sampling scheme proposed by [[1]](https://arxiv.org/pdf/1804.02772.pdf). Repuslive point processes are a formalism to sample a susbet of points that Zhang *et al.* show can reduce the variance of stochastic gradient estimates. Poisson disk sampling is one such repulsive point process that has a small overhead compared to similar schemes such as determinantal point processes [[2]](https://arxiv.org/abs/1705.00607).\n",
    "\n",
    "\n",
    "[[1]](https://arxiv.org/abs/1804.02772) C. Zhang *et al.*, Active Mini-Batch Samplingusing Repulsive Point Processes, arXiv:1804.02772, (2018).\n",
    "\n",
    "[[2]](https://arxiv.org/abs/1705.00607) C. Zhang *et al.*, Determinantal Point Processes for Mini-Batch Diversification, arXiv:1705.00607, (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sampling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5b80d59636a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msampling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpds\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPoissonDiskSampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sampling'"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '/path/to/application/app/folder')\n",
    "\n",
    "import file\n",
    "from sampling.pds import PoissonDiskSampling\n",
    "import matplotlib.pyplot as pltt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pydoc import locate\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy problem\n",
    "\n",
    "Let's imagine a toy classifier $\\mathbb{R}^2\\rightarrow\\mathbb{Z}$ that assigns a class label $y$ by:\n",
    "\n",
    "\\begin{equation}\n",
    "y = \\begin{cases}\n",
    "0 &, x^2 > \\sin(x^1) \\\\\n",
    "1 &, \\mathrm{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation},\n",
    "\n",
    "where $\\mathbf{x}=(x^1,x^2)\\in\\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_classifier = lambda _x : 0 if _x[1]>np.sin(_x[0]) else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a set of training examples with a bias away from the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate toy data\n",
    "X = np.random.multivariate_normal(mean=[0,2],cov=[[2*np.pi,0],[0,np.pi]],size=200)\n",
    "\n",
    "# true labels\n",
    "y = np.asarray([true_classifier(_x) for _x in X])\n",
    "\n",
    "def plot_train_data(X,y,_ax):\n",
    "    _ax.plot(X[np.where(y==0)[0]][:,0],X[np.where(y==0)[0]][:,1],\\\n",
    "             marker=\"o\",color=\"blue\",linestyle=\"none\",label=\"0\",\\\n",
    "             markerfacecolor=\"none\",alpha=0.4)\n",
    "    _ax.plot(X[np.where(y==1)[0]][:,0],X[np.where(y==1)[0]][:,1],\\\n",
    "            marker=\"o\",markerfacecolor=\"none\",linestyle=\"none\",\\\n",
    "            color=\"red\",label=\"1\",alpha=0.4)\n",
    "\n",
    "    _ax.plot(np.linspace(-np.pi*2,np.pi*2,100),\\\n",
    "            np.sin(np.linspace(-np.pi*2,np.pi*2,100)),color=\"orange\",\\\n",
    "            linestyle=\"--\",label=\"true decision boundary\")\n",
    "    \n",
    "    _ax.set_xlabel(\"x^1\")\n",
    "    _ax.set_ylabel(\"x^2\")\n",
    "    _ax.legend()\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(10,6))    \n",
    "    \n",
    "    \n",
    "plot_train_data(X,y,ax)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1: Categorical labelled data and the true decision boundary.\n",
    "\n",
    "### Sampling variations\n",
    "\n",
    "Zhang *et al.* [[1]](https://arxiv.org/abs/1804.02772) cover a number of variations in the mini-batch diversification via Poisson disk sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplers = {_k:PoissonDiskSampling(X,y,_k) for _k in [\"vanilla\",\"easy\",\"dense\"]}\n",
    "samples = {_k:samplers[_k].sample(20) for _k in samplers.keys()}\n",
    "keys = list(samplers.keys())\n",
    "\n",
    "\n",
    "fig,axes = plt.subplots(nrows=2,ncols=2,figsize=(18,12))\n",
    "\n",
    "for ii,[rr,cc] in enumerate(itertools.product(range(2),range(2))):\n",
    "    if ii==3:\n",
    "        axes[rr][cc].set_xticks([])\n",
    "        axes[rr][cc].set_yticks([])\n",
    "        break\n",
    "    # sampled points\n",
    "    axes[rr][cc].plot(samples[keys[ii]][0][:,0],samples[keys[ii]][0][:,1],marker=\"x\",linestyle=\"none\",\\\n",
    "            color=\"black\")\n",
    "\n",
    "    # show complete train set\n",
    "    plot_train_data(X,y,axes[rr][cc])\n",
    "\n",
    "    # show exclusion zones\n",
    "    _ = [axes[rr][cc].add_artist(plt.Circle(_x, samplers[keys[ii]].r, color='grey',alpha=0.2)) \\\n",
    "            for _x in samples[keys[ii]][0]]\n",
    "    \n",
    "    axes[rr][cc].set_title(keys[ii])\n",
    "\n",
    "#ax.set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2: A single mini-batch sample of 20 points has been drawn from the complete set of categorical data using the various Poisson disk sampling (PDS) methods described by Zhang *et al.* as \"vanilla\", \"easy\" and \"dense\". In vanilla PDS, all points repulse equally, independent on their mingling index. Unlike vanilla PDS, the easy variant sets the repulsion for points with a mingling index greater then 0, to 0. Statsitically, easy PDS samples more difficult points (closer to the decision boundary) then vanilla PDS. Dense PDS draws new points based on arbitrary categorical distributions of the mingling index for each point. This allows for an arbitrary bias towards easy of difficult to classify points, that can be changed actively throughout the stochastic gradient descent.\n",
    "\n",
    "\n",
    "Lets calculate a minimum cross-entropy estimate \n",
    "\n",
    "\\begin{equation}\n",
    "L(\\mathbf{z}) = -\\sum_{i} p(t_i) \\ln p(y_i | \\mathbf{x}_i,\\mathbf{z}),\n",
    "\\end{equation}\n",
    "where $p(t_i)$ are Dirac delta functions centred on observed labels $t_i$ and $p(y_i | \\mathbf{x}_i,\\mathbf{z})$ is the amortized categorical distribution resulting from a fully-connected feed-forward neural network with latent variables $\\mathbf{z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier():\n",
    "    def __init__(self,number_nodes=[5,5],maxiter=10,Nsample=20,initial_learning_rate=1e-3,decay_learning_rate=1e-1):\n",
    "        # dimension of input\n",
    "        self.dimX = 2\n",
    "        \n",
    "        # number of categories\n",
    "        self.dimY = 2\n",
    "        \n",
    "        # nodes per layer\n",
    "        self.number_nodes = number_nodes\n",
    "        \n",
    "        # non-linear function to apply to all but final output layer\n",
    "        self.activation_type = \"sigmoid\"\n",
    "        \n",
    "        # number of samples in mini-batch\n",
    "        self.Nsample = Nsample\n",
    "        \n",
    "        # learning rate for SGD\n",
    "        self.lr = initial_learning_rate\n",
    "        \n",
    "        # pre-factor for learning rate\n",
    "        self.init_learning_rate = initial_learning_rate\n",
    "        \n",
    "        # exponential decay factor for learning rate\n",
    "        self.decay_learning_rate = decay_learning_rate\n",
    "        \n",
    "        # number of mini-batch iterations\n",
    "        self.maxiter = maxiter\n",
    "        \n",
    "        # define conditional dependencies\n",
    "        self._init_graph()\n",
    "        \n",
    "    def _init_graph(self):\n",
    "        # nodes per layer including input and output\n",
    "        nodes = [self.dimX]+list(self.number_nodes)+[self.dimY]\n",
    "                        \n",
    "        with tf.variable_scope(\"observed_variables\"):\n",
    "            # unknown mini-batch size\n",
    "            self.X = tf.placeholder(tf.float64,[None,self.dimX],name=\"X\")\n",
    "            self.T = tf.placeholder(tf.float64,[None,self.dimY],name=\"T\")\n",
    "        \n",
    "        with tf.variable_scope(\"latent_variables\"):\n",
    "            # list of weights for each successive layer in MLP\n",
    "            self.weights = [tf.Variable(tf.random.normal(shape=(nodes[ii],nodes[ii+1]),dtype=tf.float64),\\\n",
    "                    name=\"w{}\".format(ii)) for ii in range(len(self.number_nodes)+1)]\n",
    "\n",
    "            # biases\n",
    "            self.biases = [tf.Variable(tf.zeros(shape=(nodes[ii+1]),dtype=tf.float64),name=\"b{}\".format(ii)) \\\n",
    "                    for ii in range(len(self.number_nodes)+1)]\n",
    "         \n",
    "        # [None,self.dimY] array of category probabilities\n",
    "        self._construct_model()\n",
    "\n",
    "        # objective to minimize stochastically wrt latent parameters in net\n",
    "        self._construct_loss()\n",
    "        \n",
    "        # start tf session\n",
    "        self.session = tf.Session()\n",
    "        \n",
    "        # create instances of variables\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        \n",
    "                \n",
    "    def _construct_model(self):\n",
    "        # activation function for intermediate layers\n",
    "        act1 = {\"sigmoid\":tf.math.sigmoid,\"tanh\":tf.math.tanh}[self.activation_type]\n",
    "        \n",
    "        # final layer\n",
    "        idx = len(self.weights)-1\n",
    "        \n",
    "        # non-linear operation on output layer ii\n",
    "        act_func = lambda _x,ii : act1(_x) if ii!=idx else tf.math.softmax(_x)\n",
    "        \n",
    "        \n",
    "        layer_output = act_func(tf.add(tf.matmul(self.X,self.weights[0]),self.biases[0]),0)\n",
    "        for ii in range(1,len(self.weights)):\n",
    "            layer_output = act_func(tf.add(tf.matmul(layer_output,self.weights[ii]),self.biases[ii]),ii)   \n",
    "        self.y = layer_output\n",
    "     \n",
    "        \n",
    "    def _construct_loss(self):\n",
    "        # use cross entropy\n",
    "        self._loss = tf.reduce_mean(-tf.reduce_sum(self.T * tf.log(self.y), reduction_indices=[1]))\n",
    "   \n",
    "        # SGD optimizer\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.lr)\n",
    "        \n",
    "        # apply stochastic gradients to minimize objective\n",
    "        self.trainop = self.optimizer.apply_gradients(zip(tf.gradients(self._loss,\\\n",
    "                tf.trainable_variables()),tf.trainable_variables()))\n",
    "      \n",
    "        \n",
    "    def fit(self,X,y,minibatch_sampler,supp_method=None):\n",
    "        if not isinstance(minibatch_sampler,PoissonDiskSampling): raise Exception(\"Expecting a PDS instance.\")\n",
    "        \n",
    "        # attach as attributes for any supplemental methods that are supplied\n",
    "        self.Xtrain = X\n",
    "        self.ytrain = y\n",
    "        \n",
    "        writer = tf.summary.FileWriter('./graphs', self.session.graph)\n",
    "             \n",
    "        self.loss = np.zeros(self.maxiter)\n",
    "        \n",
    "        for ii in range(self.maxiter):           \n",
    "            Xsample,ysample = minibatch_sampler.sample(self.Nsample)\n",
    "            \n",
    "            feed = {self.X:Xsample,self.T:self._reformatY(ysample)}\n",
    "            \n",
    "            _,loss = self.session.run([self.trainop,self._loss], feed)\n",
    "\n",
    "            \n",
    "            # book keeping\n",
    "            self.loss[ii] = loss\n",
    "            self.iter = ii            \n",
    "            \n",
    "          \n",
    "            # update learning rate\n",
    "            self._update_learning_rate()\n",
    "            \n",
    "            if not supp_method is None:\n",
    "                # any supplementary methods\n",
    "                supp_method(self)\n",
    "            \n",
    "   \n",
    "    def predict(self,X):\n",
    "        # [N,self.dimY] probabilities\n",
    "        [res] = self.session.run([self.y],{self.X:X})\n",
    "\n",
    "        # label highest probable category\n",
    "        res = np.asarray([np.argmax(_y) for _y in res],dtype=np.int)\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        # exponential decay\n",
    "        self.lr = self.init_learning_rate*np.exp(-self.iter*self.decay_learning_rate)\n",
    "    \n",
    "    def _reformatY(self,y):\n",
    "        # parse a [N,] array of categorical data into a [N,self.dimY] array of binary values\n",
    "        \n",
    "        Y = np.zeros((y.shape[0],self.dimY))\n",
    "\n",
    "        \n",
    "        for ii in range(y.shape[0]):\n",
    "            Y[ii,y[ii]] = 1\n",
    "            \n",
    "        return Y  \n",
    "    \n",
    "    def set_test_data(self,Xtest,ytest):\n",
    "        \"\"\"\n",
    "        Attach test data to evaluate performance metrics in supplementary methods that are\n",
    "        attached to run during stochastic gradient descent.\n",
    "        \"\"\"\n",
    "        self.Xtest = Xtest\n",
    "        self.ytest = ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using mini-batch sampling, the cross entropy is stochastically minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate parameters effect the gradient pre-factor in latent variable updates\n",
    "model = MLPClassifier(maxiter=10000,initial_learning_rate=1e-1,decay_learning_rate=1e-3)\n",
    "model.fit(X,y,PoissonDiskSampling(X,y,method=\"easy\"))\n",
    "\n",
    "fig,axes = plt.subplots(nrows=1,ncols=3,figsize=(16,4))\n",
    "\n",
    "# cross entryop minimization\n",
    "axes[0].plot(np.arange(len(model.loss)),model.loss)\n",
    "axes[0].set_xlabel(\"iteration number\")\n",
    "axes[0].set_ylabel(\"cross entropy\")\n",
    "\n",
    "# true boundary\n",
    "plot_train_data(X,y,axes[1])\n",
    "axes[1].set_title(\"True labels\")\n",
    "\n",
    "# infered boundary\n",
    "plot_train_data(X,model.predict(X),axes[2])\n",
    "axes[2].set_title(\"Infered labels\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# clear tf graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3: The left-most sub-figure shows the cross entropy for a 2 5-node layer feed-forward neural network with the iteration number of a mini-batch stochastic gradient descent. The middle and right-most sub-figures show the true and infered labels, respectively, following $10^4$ iterations of the gradient descent.\n",
    "\n",
    "### Comparing sampling biases for mini-batch diversification\n",
    "\n",
    "To compare the effect of different biasing schemes, we can compute the accuracy score for a held-out set of test data close to the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate toy data\n",
    "Xtest = np.random.uniform(low=[-6,-1],high=[6,1],size=(500,2))\n",
    "\n",
    "# true labels\n",
    "ytest = np.asarray([true_classifier(_x) for _x in Xtest])\n",
    "\n",
    "fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(8,6))\n",
    "\n",
    "plot_train_data(Xtest,ytest,ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4: A uniformly sampled set of test labels close to the decision boundary. \n",
    "\n",
    "\n",
    "To calculate the model accuracy during stochastic optimisation, lets attach a supplemental method to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_score(self):\n",
    "    # only evaluate every step_interval'th iteration\n",
    "    step_interval = 1\n",
    "    \n",
    "    if np.isclose(np.mod(self.iter,step_interval),0):\n",
    "        try:\n",
    "            y_pred = self.predict(self.Xtest)\n",
    "        except AttributeError:\n",
    "            raise Exception(\"Attach test data to class through self.set_test_data() before calling this method\")\n",
    "\n",
    "        score = accuracy_score(y_true=self.ytest,y_pred=y_pred)\n",
    "\n",
    "        try:\n",
    "            self.test_score.append(score)\n",
    "        except AttributeError:\n",
    "            # first call\n",
    "            self.test_score = [score]\n",
    "        \n",
    "test_scores = {_k:None for _k in [\"vanilla\",\"easy\",\"dense\",\"anneal\"]}\n",
    "        \n",
    "for _k in test_scores.keys():\n",
    "    model = MLPClassifier(maxiter=1000,initial_learning_rate=1e-1,decay_learning_rate=1e-3)\n",
    "\n",
    "    # attach test data\n",
    "    model.set_test_data(Xtest,ytest)\n",
    "\n",
    "    # attach supplemetary method, called once per iteration of gradient descent\n",
    "    model.fit(X,y,PoissonDiskSampling(X,y,method=_k),evaluate_test_score)\n",
    "\n",
    "    # clear tf graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # accuracy score with iteration number\n",
    "    test_scores[_k] = model.test_score\n",
    "    \n",
    "# compare accuracy score of all PDS methods\n",
    "fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(8,6))\n",
    "\n",
    "_ = [ax.plot(np.arange(len(test_scores[_k])),test_scores[_k],label=_k,alpha=0.7) for _k in test_scores.keys()]\n",
    "\n",
    "ax.set_xlabel(\"iteration number\")\n",
    "ax.set_ylabel(\"test accuracy score\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 5: The accuracy score evaluated on a a large test set close to the boundary is shown for every PDS variation proposed in *Zhang et al.* . We note that the choice of categorical distribution coefficients for sampling mingling index values for the dense and anneal methods is somewhat heuristic and should probably be refined on a data-specific basis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
